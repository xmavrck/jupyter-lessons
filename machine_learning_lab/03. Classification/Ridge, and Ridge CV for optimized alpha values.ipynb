{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../logo.png\",width=200,height=60>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing $\\alpha$ value for Ridge\n",
    "\n",
    "Ridge is nothing but Regularized version of Least Squares. Often we don't know which value of $\\alpha$ would give us the best results. What we can do is try out with different values and then select the one with the best cross-validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create made up data\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=3, effective_rank=2, noise=10)\n",
    "# effective rank is the number of variables that \n",
    "# are enough to describe the input variables. Hence most \n",
    "# of the input data will be linear combination of these\n",
    "# singluar vectors. Rest of the variables will be fairly\n",
    "# irrelevant to the output. \n",
    "\n",
    "# noise is the standard deviation of the gaussian applied to\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_grid = np.linspace(0.1, 1, 10)\n",
    "alpha_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Costs: [[2.14472645e+01 1.07975748e+01 4.75876414e+00 1.58200543e+00\n",
      "  2.24238378e-01 4.03349487e-02 6.19464371e-01 1.69378453e+00\n",
      "  3.08537575e+00 4.67431768e+00]\n",
      " [8.05372323e+01 8.38260387e+01 8.54738373e+01 8.61833547e+01\n",
      "  8.63413419e+01 8.61706111e+01 8.58041286e+01 8.53233981e+01\n",
      "  8.47793690e+01 8.42043106e+01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "clf = RidgeCV(alphas=alpha_grid, store_cv_values=True)\n",
    "clf.fit(X, y)\n",
    "print(\"Best alpha: {}\".format(clf.alpha_))\n",
    "print(\"Costs: {}\".format(clf.cv_values_[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will internally run multiple iterations of cross validation and select the alpha with least average cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring with Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "l1_error = make_scorer(mean_absolute_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.2\n",
      "Costs: [[-13.78000134 -12.43484854 -11.33034137 -10.40666017  -9.62242035\n",
      "   -8.94804657  -8.36182161  -7.84742743  -7.39235843  -6.98686516]\n",
      " [ -1.08635116  -1.2677534   -1.35730348  -1.39559635  -1.40410149\n",
      "   -1.39490997  -1.3751491   -1.34916383  -1.31966859  -1.28838798]]\n"
     ]
    }
   ],
   "source": [
    "clf = RidgeCV(alphas=alpha_grid, store_cv_values=True, scoring=l1_error)\n",
    "clf.fit(X, y)\n",
    "print(\"Best alpha: {}\".format(clf.alpha_))\n",
    "print(\"Costs: {}\".format(clf.cv_values_[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the best alpha is same, the error are relatively smaller. This is because of the fact that by defualt RMSE is used (in the previous example) and in the last example we've used MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same technique is also applicable to __Lasso__ and __LassoCV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&copy; 2018 Stacklabs<p>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
